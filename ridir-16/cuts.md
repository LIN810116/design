# cuts.md

More text available to Databrary search engine, the more relevant and powerful the searches can become.
Establishes framework for future extensions that would link to online (HTML-based) versions of published papers for data mining (e.g., neurosynth.org).
Further, Acknowledges growth in text-based data mining and analysis procedures.
Without access to text-based annotations of video segments, these tools can't yet be readily applied to video.

Behavior, thought, feeling, and communication unfold in real time.
No other measure captures the dynamics of behavior like video.
Video and before it, film, has formed the core of research in human development for decades.

- uniquely captures complexity of behavior
- cheap to collect, largely self-documenting
- widely used
- flexible, can be used in many contexts
- high reuse potential

Perhaps the most important challenge is cultural.
Community practices must change. Most researchers in the education, learning, and developmental sciences do not reuse their own videos or videos collected by other researchers; they neither recognize nor endorse the value of open sharing.
Contributing data is anathema and justifications against sharing are many. Researchers cite intellectual property and privacy issues, the lack of data sharing requirements from funding agencies, and fears about the misuse, misinterpretation, or professional harm that might come from sharing (Ascoli, 2006b; Ferguson, 2014).
Data sharing diverts energy and resources from scholarly activities that are more heavily and frequently rewarded. These barriers must be overcome to make video data sharing a scientific norm.

#### Project 1.4 Develop mechanisms for selecting and storing for later analysis found video segments.

(*I'm not sure any of this is in scope.*)

Once the search engine is able to find video segments and return them to the user in a way that enables browsing or previewing, we will need to develop a mechanism that allows users to select videos for subsequent detailed evaluation.
From the user's point of view, we envision an interface with checkboxes or similar functionality, possibly including filters that, for example, select the top 10 or 50 hits.
Then, we will need to create a workspace where a user can preview, comment upon,  and further evaluate the videos found in the prior search.
Workspaces will build on Databrary's existing "volume" functionality, where volumes are containers within which users can store groups of materials.
Workspaces, like volumes, will be linked to metadata like a project description, funding source, and have places to store materials like coding manuals, statistical analyses, etc.
The specific search and filter criteria will be stored so that these might be reused in the future.
Workspaces and volumes can be shared with other specific researchers a user selects or at some future date more widely.
We envision needing to modify Databrary's existing volume interface to indicate the source(s) of materials, and perhaps the search term(s) that generated the materials.
There will also need to be provisions in place to allow any annotations, comments, notes, or codes a user adds to materials "borrowed" from Databrary's shared videos to be linked back to the original sources.
A user who searches for videos meeting specific terms will not be required to share private annotations or codes, but those who do make their annotations or codes available will allow future searchers to benefit from the work of past searchers.
Also, users will be able to discard specific videos from a workspace or to delete a workspace if it is not longer in use.
Databrary's existing volume audit/tracking tools will be modified to allow users to review the history of their own workspaces/volumes.

Project 4.1 will focus on creating mechanisms that track data provenance -- segment HH:MM:SSS.mmm-HH:MM:SSS.mmm from video nnn from volume vvv and user uuu.
